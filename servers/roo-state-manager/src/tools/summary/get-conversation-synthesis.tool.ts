/**
 * MCP Tool pour r√©cup√©rer la synth√®se d'une conversation individuelle
 * 
 * Ce tool expose l'acc√®s aux r√©sultats de synth√®se via l'interface MCP,
 * permettant la r√©cup√©ration des analyses format√©es √† partir du SynthesisOrchestratorService.
 *
 * SDDD Phase 3 : Int√©gration LLM r√©elle avec pipeline complet de synth√®se
 *
 * @author Roo Code v4 - SDDD Phase 3
 * @version 3.0.0
 */

import { Tool } from '@modelcontextprotocol/sdk/types.js';
import { StateManagerError } from '../../types/errors.js';
import { ConversationAnalysis } from '../../models/synthesis/SynthesisModels.js';
import { ConversationSkeleton } from '../../types/conversation.js';
import { SynthesisOrchestratorService } from '../../services/synthesis/SynthesisOrchestratorService.js';
import { NarrativeContextBuilderService } from '../../services/synthesis/NarrativeContextBuilderService.js';
import { LLMService } from '../../services/synthesis/LLMService.js';
import fs from 'fs/promises';
import path from 'path';
import { RooStorageDetector } from '../../utils/roo-storage-detector.js';

/**
 * Arguments du tool get_conversation_synthesis
 */
export interface GetConversationSynthesisArgs {
    /** ID de la t√¢che pour laquelle r√©cup√©rer la synth√®se */
    taskId: string;
    
    /** Chemin optionnel pour sauvegarder la sortie. Si omis, le r√©sultat est retourn√© */
    filePath?: string;
    
    /** Format de sortie : 'json' pour l'objet complet, 'markdown' pour la section narrative */
    outputFormat?: 'json' | 'markdown';
}

/**
 * D√©finition du tool MCP selon les sp√©cifications de l'API v3
 */
export const getConversationSynthesisTool: Tool = {
    name: "get_conversation_synthesis",
    description: "R√©cup√®re ou exporte le r√©sultat de la synth√®se pour une seule conversation via LLM r√©el OpenAI (Phase 3).",
    inputSchema: {
        type: "object",
        properties: {
            taskId: {
                type: "string",
                description: "ID de la t√¢che pour laquelle r√©cup√©rer la synth√®se"
            },
            filePath: {
                type: "string",
                description: "Chemin optionnel pour sauvegarder la sortie. Si omis, le r√©sultat est retourn√©."
            },
            outputFormat: {
                type: "string",
                enum: ["json", "markdown"],
                default: "json",
                description: "Le format de sortie. 'markdown' ne retourne que la section narrative finale."
            }
        },
        required: ["taskId"]
    }
};

/**
 * Impl√©mentation du handler pour le tool get_conversation_synthesis
 * 
 * Phase 3 : Int√©gration LLM r√©elle avec SynthesisOrchestratorService
 */
export async function handleGetConversationSynthesis(
    args: GetConversationSynthesisArgs,
    getConversationSkeleton: (taskId: string) => Promise<ConversationSkeleton | null>
): Promise<string | ConversationAnalysis> {
    try {
        // Valider les arguments
        if (!args.taskId) {
            throw new StateManagerError(
                'taskId est requis',
                'VALIDATION_FAILED',
                'GetConversationSynthesisTool',
                { missingParam: 'taskId' }
            );
        }

        const outputFormat = args.outputFormat || 'json';
        
        // Phase 3 : Appel au pipeline LLM r√©el via SynthesisOrchestratorService
        const realAnalysis = await generateRealSynthesis(args.taskId, getConversationSkeleton);
        
        // Si un chemin de fichier est sp√©cifi√©, √©crire dans le fichier
        if (args.filePath) {
            await writeAnalysisToFile(realAnalysis, args.filePath, outputFormat);
            return `‚úÖ Synth√®se LLM r√©elle de la t√¢che '${args.taskId}' export√©e vers '${args.filePath}' au format ${outputFormat}`;
        }
        
        // Sinon retourner le contenu selon le format demand√©
        if (outputFormat === 'markdown') {
            return realAnalysis.synthesis.finalTaskSummary;
        } else {
            return realAnalysis;
        }
        
    } catch (error) {
        if (error instanceof StateManagerError) {
            throw error;
        }
        const errorMessage = error instanceof Error ? error.message : 'Erreur inconnue';
        throw new StateManagerError(
            `Erreur lors de la r√©cup√©ration de la synth√®se: ${errorMessage}`,
            'SYNTHESIS_RETRIEVAL_FAILED',
            'GetConversationSynthesisTool',
            { taskId: args.taskId, originalError: errorMessage }
        );
    }
}

/**
 * G√©n√®re une synth√®se r√©elle via le pipeline LLM complet
 * Phase 3 : Int√©gration LLM avec SynthesisOrchestratorService
 *
 * @param taskId ID de la t√¢che pour laquelle g√©n√©rer l'analyse
 * @param getConversationSkeleton Fonction pour r√©cup√©rer le skeleton de conversation
 * @returns Promise de l'analyse g√©n√©r√©e par LLM
 */
async function generateRealSynthesis(
    taskId: string,
    getConversationSkeleton: (taskId: string) => Promise<ConversationSkeleton | null>
): Promise<ConversationAnalysis> {
    // V√©rifier que la conversation existe
    const skeleton = await getConversationSkeleton(taskId);
    if (!skeleton) {
        throw new StateManagerError(
            `Conversation non trouv√©e: ${taskId}`,
            'CONVERSATION_NOT_FOUND',
            'GetConversationSynthesisTool',
            { taskId }
        );
    }

    // Instancier les services avec configuration par d√©faut
    const llmService = new LLMService();
    
    // ‚úÖ SDDD FIX: Cr√©er et peupler le cache avec les vraies conversations
    const conversationCache = new Map<string, ConversationSkeleton>();
    await populateConversationCache(conversationCache);
    
    const contextBuilder = new NarrativeContextBuilderService({
        synthesisBaseDir: '.skeletons/synthesis',
        condensedBatchesDir: '.skeletons/synthesis/batches',
        maxContextSizeBeforeCondensation: 50000,
        defaultMaxDepth: 10
    }, conversationCache);
    
    // Injecter la fonction de r√©cup√©ration des conversations dans le context builder
    // Note: En production, cette fonction sera fournie par le serveur principal
    (contextBuilder as any).getConversationSkeleton = getConversationSkeleton;
    
    const orchestrator = new SynthesisOrchestratorService(contextBuilder, llmService, {
        synthesisOutputDir: '.skeletons/synthesis',
        maxContextSize: 50000,
        maxConcurrency: 3,
        defaultLlmModel: 'gpt-4-turbo-synthesis'
    });

    try {
        // Appel du pipeline complet : contexte + LLM
        console.log(`üöÄ [MCP Tool] D√©marrage synth√®se LLM pour ${taskId}...`);
        const analysis = await orchestrator.synthesizeConversation(taskId);
        console.log(`‚úÖ [MCP Tool] Synth√®se LLM termin√©e pour ${taskId}`);
        
        return analysis;
        
    } catch (error) {
        console.error(`‚ùå [MCP Tool] Erreur synth√®se ${taskId}:`, error);
        
        // Fallback : retourner une analyse d'erreur coh√©rente
        const fallbackAnalysis: ConversationAnalysis = {
            taskId,
            analysisEngineVersion: "3.0.0-error",
            analysisTimestamp: new Date().toISOString(),
            llmModelId: "error-fallback",
            contextTrace: {
                rootTaskId: taskId,
                parentTaskId: undefined,
                previousSiblingTaskIds: []
            },
            objectives: { error: true, message: error instanceof Error ? error.message : 'Unknown error' },
            strategy: { error: true },
            quality: { error: true },
            metrics: { error: error instanceof Error ? error.message : 'Unknown error' },
            synthesis: {
                initialContextSummary: "Erreur lors de la construction du contexte narratif",
                finalTaskSummary: `Erreur lors de la synth√®se LLM: ${error instanceof Error ? error.message : 'Unknown error'}`
            }
        };
        
        return fallbackAnalysis;
    }
}

/**
 * DEPRECATED: G√©n√®re une analyse MOCK (conserv√©e pour r√©f√©rence)
 * Remplac√©e par generateRealSynthesis() en Phase 3
 */
async function generateMockAnalysis(taskId: string): Promise<ConversationAnalysis> {
    const timestamp = new Date().toISOString();
    
    // G√©n√©rer des donn√©es MOCK coh√©rentes avec SynthesisModels.ts
    const mockAnalysis: ConversationAnalysis = {
        // M√©tadonn√©es de l'analyse
        taskId: taskId,
        analysisEngineVersion: "1.0.0-mock",
        analysisTimestamp: timestamp,
        llmModelId: "gpt-4-mock",
        
        // Trace de contexte (MOCK)
        contextTrace: {
            rootTaskId: generateMockRootTaskId(taskId),
            parentTaskId: generateMockParentTaskId(taskId),
            previousSiblingTaskIds: generateMockSiblingIds(taskId, 2)
        },
        
        // Sections d'analyse structur√©e (MOCK)
        objectives: {
            primaryGoal: "R√©soudre un probl√®me technique sp√©cifique",
            secondaryGoals: ["Documenter la solution", "Tester l'impl√©mentation"],
            complexityLevel: "medium",
            estimatedDuration: "2-4 heures"
        },
        
        strategy: {
            approach: "Analyse syst√©matique et impl√©mentation incr√©mentale",
            methodology: "SDDD (Semantic-Documentation-Driven-Design)",
            toolsUsed: ["codebase_search", "read_file", "apply_diff", "write_to_file"],
            phasedExecution: true
        },
        
        quality: {
            completionScore: 8.5,
            codeQuality: "high",
            documentationQuality: "excellent",
            testCoverage: "good",
            adherenceToPatterns: "excellent"
        },
        
        metrics: {
            totalMessages: 47,
            userMessages: 12,
            assistantMessages: 35,
            toolCallsCount: 28,
            filesModified: 5,
            linesOfCodeAdded: 342,
            linesOfCodeModified: 89,
            timeSpent: "3h 25m",
            tokenUsage: {
                input: 45230,
                output: 12890,
                total: 58120
            }
        },
        
        // Synth√®se narrative incr√©mentale
        synthesis: {
            initialContextSummary: generateMockInitialContext(taskId),
            finalTaskSummary: generateMockFinalSummary(taskId)
        }
    };
    
    return mockAnalysis;
}

/**
 * G√©n√®re un contexte initial MOCK r√©aliste
 */
function generateMockInitialContext(taskId: string): string {
    return `
**Contexte Narratif Initial pour ${taskId}**

Cette t√¢che s'inscrit dans le cadre du d√©veloppement du syst√®me de synth√®se de conversations Roo. 
Le contexte pr√©alable indique une architecture modulaire √©tablie avec des services existants 
(TraceSummaryService, ExportConfigManager) suivant des patterns bien d√©finis.

**√âl√©ments de contexte collect√©s :**
- Architecture existante valid√©e et document√©e
- Patterns de d√©veloppement TypeScript identifi√©s et respect√©s  
- Structure des services avec injection de d√©pendances confirm√©e
- Outils MCP avec pattern dual export (d√©finition + handler) √©tabli

**Objectifs contextuels :**
L'impl√©mentation doit s'int√©grer transparently dans l'√©cosyst√®me existant sans rupture 
de compatibilit√©, en respectant scrupuleusement les conventions architecturales observ√©es.
`.trim();
}

/**
 * G√©n√®re un r√©sum√© final MOCK r√©aliste
 */
function generateMockFinalSummary(taskId: string): string {
    return `
**Synth√®se Finale - Phase 1 : Impl√©mentation Squelette MCP Synth√®se**

**R√©alisations accomplies :**
‚úÖ **Architecture int√©gr√©e** : Tous les composants suivent les patterns existants (singleton, DI, dual export)
‚úÖ **Mod√®les de donn√©es** : SynthesisModels.ts complet avec 15+ interfaces TypeScript document√©es
‚úÖ **Services squelette** : 3 services cr√©√©s (SynthesisOrchestrator, NarrativeContextBuilder, LLMService)
‚úÖ **Outil MCP fonctionnel** : get_conversation_synthesis impl√©ment√© avec version MOCK
‚úÖ **Documentation SDDD** : Tra√ßabilit√© compl√®te des d√©cisions et d√©couvertes architecturales

**Qualit√© technique :**
- **Respect des patterns** : 100% conforme aux conventions roo-state-manager
- **Documentation** : JSDoc complet sur toutes les interfaces et m√©thodes publiques
- **Structure modulaire** : Organisation claire models/services/tools selon architecture cible
- **Extensibilit√©** : Conception pr√©par√©e pour les phases 2-4 d'impl√©mentation

**Impact strat√©gique :**
Cette phase √©tablit les fondations solides pour l'impl√©mentation compl√®te du syst√®me de synth√®se. 
L'int√©gration transparente dans l'architecture existante garantit la maintenabilit√© et l'√©volutivit√©.
La validation MCP confirme que l'infrastructure est op√©rationnelle pour les phases suivantes.

**Prochaines √©tapes recommand√©es :**
- Phase 2 : Impl√©mentation de la construction de contexte narratif
- Phase 3 : Int√©gration LLM avec openai-node
- Phase 4 : Traitement par lots et exports avanc√©s

*Note : Cette synth√®se est g√©n√©r√©e par la version MOCK Phase 1 √† des fins de validation d'infrastructure.*
`.trim();
}

/**
 * √âcrit l'analyse dans un fichier selon le format sp√©cifi√©
 */
async function writeAnalysisToFile(
    analysis: ConversationAnalysis,
    filePath: string,
    format: 'json' | 'markdown'
): Promise<void> {
    // Cr√©er le r√©pertoire si n√©cessaire
    const dir = path.dirname(filePath);
    await fs.mkdir(dir, { recursive: true });
    
    let content: string;
    
    if (format === 'markdown') {
        content = `# Synth√®se de Conversation - ${analysis.taskId}

**Analyse g√©n√©r√©e le :** ${analysis.analysisTimestamp}  
**Moteur :** ${analysis.analysisEngineVersion}  
**Mod√®le LLM :** ${analysis.llmModelId}

---

## Contexte Initial

${analysis.synthesis.initialContextSummary}

---

## Synth√®se Finale

${analysis.synthesis.finalTaskSummary}

---

## M√©triques

- **Messages totaux :** ${analysis.metrics.totalMessages}
- **Fichiers modifi√©s :** ${analysis.metrics.filesModified || 'N/A'}
- **Temps estim√© :** ${analysis.metrics.timeSpent || 'N/A'}
- **Score de qualit√© :** ${analysis.quality.completionScore}/10

---

*Synth√®se g√©n√©r√©e automatiquement par roo-state-manager MCP Synthesis v${analysis.analysisEngineVersion}*
`;
    } else {
        content = JSON.stringify(analysis, null, 2);
    }
    
    await fs.writeFile(filePath, content, 'utf-8');
}

// =========================================================================
// FONCTIONS UTILITAIRES MOCK
// =========================================================================

/**
 * G√©n√®re un ID de t√¢che racine MOCK
 */
function generateMockRootTaskId(taskId: string): string {
    // Simuler une relation hi√©rarchique coh√©rente
    return `root_${taskId.substring(0, 8)}_main`;
}

/**
 * G√©n√®re un ID de t√¢che parent MOCK
 */
function generateMockParentTaskId(taskId: string): string | undefined {
    // 70% des t√¢ches ont un parent dans la simulation
    return Math.random() > 0.3 ? `parent_${taskId.substring(0, 6)}_ctx` : undefined;
}

/**
 * G√©n√®re des IDs de t√¢ches s≈ìurs MOCK
 */
function generateMockSiblingIds(taskId: string, count: number): string[] {
    const siblings: string[] = [];
    for (let i = 0; i < count; i++) {
        siblings.push(`sibling_${taskId.substring(0, 6)}_${i + 1}`);
    }
    return siblings;
}

/**
 * SDDD Phase 3 FIX: Popule le cache de conversations avec la m√™me logique que le serveur principal
 * Bas√© sur handleBuildSkeletonCache() du serveur principal (index.ts:894)
 *
 * @param conversationCache Cache √† peupler
 */
async function populateConversationCache(conversationCache: Map<string, ConversationSkeleton>): Promise<void> {
    console.log('üîÑ [SDDD FIX] Chargement du cache des conversations...');
    
    try {
        const locations = await RooStorageDetector.detectStorageLocations();
        if (locations.length === 0) {
            console.warn('‚ö†Ô∏è Aucun emplacement de stockage Roo d√©tect√©');
            return;
        }

        let totalLoaded = 0;
        const SKELETON_CACHE_DIR_NAME = '.skeletons';
        
        for (const location of locations) {
            const skeletonsCacheDir = path.join(location, SKELETON_CACHE_DIR_NAME);
            
            try {
                const skeletonFiles = await fs.readdir(skeletonsCacheDir);
                const jsonFiles = skeletonFiles.filter(file => file.endsWith('.json'));
                
                for (const fileName of jsonFiles) {
                    const skeletonPath = path.join(skeletonsCacheDir, fileName);
                    
                    try {
                        let skeletonContent = await fs.readFile(skeletonPath, 'utf-8');
                        
                        // Supprimer le BOM UTF-8 si pr√©sent (m√™me logique que le serveur)
                        if (skeletonContent.charCodeAt(0) === 0xFEFF) {
                            skeletonContent = skeletonContent.slice(1);
                        }
                        
                        const skeleton: ConversationSkeleton = JSON.parse(skeletonContent);
                        if (skeleton && skeleton.taskId) {
                            conversationCache.set(skeleton.taskId, skeleton);
                            totalLoaded++;
                        }
                    } catch (parseError) {
                        console.warn(`‚ö†Ô∏è Erreur parsing skeleton ${fileName}:`, parseError);
                    }
                }
            } catch (readError) {
                console.warn(`‚ö†Ô∏è Impossible de lire le dossier ${skeletonsCacheDir}:`, readError);
            }
        }
        
        console.log(`‚úÖ [SDDD FIX] Cache charg√© avec ${totalLoaded} conversations (trouv√©es dans ${locations.length} emplacements)`);
        
    } catch (error) {
        console.error('‚ùå [SDDD FIX] Erreur lors du chargement du cache:', error);
    }
}
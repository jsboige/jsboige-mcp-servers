/**
 * LLMService - Service d'interface avec les mod√®les de langage (LLM)
 * 
 * Ce service fournit une interface unifi√©e pour les appels aux diff√©rents mod√®les LLM
 * utilis√©s dans le processus de synth√®se. Il centralise la gestion des appels,
 * l'authentification, la gestion des erreurs et les optimisations de performance.
 * 
 * Architecture wrapper autour d'openai-node avec pattern "Exception Wrapping".
 * 
 * SDDD Phase 3 : Impl√©mentation LLM compl√®te avec int√©gration OpenAI r√©elle
 *
 * @author Roo Code v4 - SDDD Phase 3
 * @version 3.0.0
 */

import { ConversationAnalysis, CondensedSynthesisBatch, ContextTrace, SynthesisNarrative } from '../../models/synthesis/SynthesisModels.js';
import getOpenAIClient from '../openai.js';
import OpenAI from 'openai';
import { zodResponseFormat } from 'openai/helpers/zod';
import { z } from 'zod';
import * as crypto from 'crypto';

// =============================================================================
// SCHEMAS ZOD POUR STRUCTURED OUTPUT
// =============================================================================

/**
 * Schema Zod pour la structure ContextTrace conforme aux models
 */
const ContextTraceSchema = z.object({
    rootTaskId: z.string(),
    parentTaskId: z.string().nullable(),
    previousSiblingTaskIds: z.array(z.string())
});

/**
 * Schema Zod pour la structure SynthesisNarrative conforme aux models
 */
const SynthesisNarrativeSchema = z.object({
    initialContextSummary: z.string(),
    finalTaskSummary: z.string()
});

/**
 * Schema Zod principal pour ConversationAnalysis - garantit la conformit√© structur√©e
 * Avec types explicites pour OpenAI structured outputs
 */
const ConversationAnalysisSchema = z.object({
    // M√©tadonn√©es de l'analyse
    taskId: z.string(),
    analysisEngineVersion: z.string(),
    analysisTimestamp: z.string(),
    llmModelId: z.string(),

    // Tra√ßabilit√© du contexte
    contextTrace: ContextTraceSchema,

    // Sections d'analyse structur√©e avec types explicites
    objectives: z.object({
        primary_goal: z.string(),
        secondary_goals: z.array(z.string()),
        success_criteria: z.array(z.string())
    }),
    strategy: z.object({
        approach: z.string(),
        tools_used: z.array(z.string()),
        methodology: z.string()
    }),
    quality: z.object({
        completeness_score: z.number(),
        clarity_score: z.number(),
        effectiveness_rating: z.string(),
        issues_found: z.array(z.string())
    }),
    metrics: z.object({
        contextLength: z.number(),
        wasCondensed: z.boolean(),
        processing_time_ms: z.number().nullable(),
        complexity_score: z.number().nullable(),
        // Nouveau : tra√ßage de l'arbre des synth√®ses incluses
        contextTree: z.object({
            currentTask: z.object({
                taskId: z.string(),
                synthesisType: z.enum(["atomic", "condensed_batch", "on_demand"]),
                includedInContext: z.boolean()
            }),
            parentTasks: z.array(z.object({
                taskId: z.string(),
                synthesisType: z.enum(["atomic", "condensed_batch", "on_demand", "missing"]),
                includedInContext: z.boolean(),
                errorMessage: z.string().nullable()
            })),
            siblingTasks: z.array(z.object({
                taskId: z.string(),
                synthesisType: z.enum(["atomic", "condensed_batch", "on_demand", "missing"]),
                includedInContext: z.boolean(),
                errorMessage: z.string().nullable()
            })),
            condensedBatches: z.array(z.object({
                batchId: z.string(),
                taskIds: z.array(z.string()),
                includedInContext: z.boolean()
            }))
        })
    }),

    // Synth√®se narrative incr√©mentale
    synthesis: SynthesisNarrativeSchema
});

/**
 * Schema pour la condensation de synth√®ses multiples
 */
const CondensedSynthesisBatchSchema = z.object({
    batchId: z.string(),
    creationTimestamp: z.string(),
    llmModelId: z.string(),
    batchSummary: z.string(),
    sourceTaskIds: z.array(z.string())
});

// =============================================================================
// CONFIGURATIONS MOD√àLES STANDARD
// =============================================================================

/**
 * Configuration par d√©faut pour GPT-4o optimis√©e pour la synth√®se
 * Note: GPT-4o supporte les structured outputs avec zodResponseFormat
 */
const DEFAULT_MODEL_CONFIG: LLMModelConfig = {
    modelId: 'gpt-4o-mini-synthesis',
    displayName: 'GPT-4o Mini (Synth√®se)',
    provider: 'openai',
    modelName: process.env.OPENAI_CHAT_MODEL_ID || 'gpt-4o-mini',
    maxTokens: 128000,
    costPerInputToken: 0.00015 / 1000,  // Prix GPT-4o-mini
    costPerOutputToken: 0.0006 / 1000,   // Prix GPT-4o-mini
    parameters: {
        temperature: 0.1
    }
};

/**
 * Configuration d'options par d√©faut pour le service
 */
const DEFAULT_SERVICE_OPTIONS: LLMServiceOptions = {
    models: [DEFAULT_MODEL_CONFIG],
    defaultModelId: 'gpt-4o-mini-synthesis',
    defaultTimeout: 60000, // 60 secondes
    maxRetries: 3,
    retryDelay: 1000,
    enableCaching: true
};

/**
 * Configuration d'un mod√®le LLM support√©.
 * Permet la gestion multi-mod√®les avec des param√®tres sp√©cifiques.
 */
export interface LLMModelConfig {
    /** Identifiant unique du mod√®le */
    modelId: string;
    
    /** Nom d'affichage du mod√®le */
    displayName: string;
    
    /** Fournisseur du mod√®le (OpenAI, Anthropic, etc.) */
    provider: 'openai' | 'anthropic' | 'azure' | 'local';
    
    /** Nom technique du mod√®le chez le fournisseur */
    modelName: string;
    
    /** Limite de tokens de contexte */
    maxTokens: number;
    
    /** Co√ªt par token d'entr√©e (pour monitoring) */
    costPerInputToken: number;
    
    /** Co√ªt par token de sortie (pour monitoring) */
    costPerOutputToken: number;
    
    /** Param√®tres sp√©cifiques au mod√®le */
    parameters: {
        temperature?: number;
    };
}

/**
 * Options globales de configuration du service LLM.
 * Contr√¥le le comportement g√©n√©ral des appels LLM.
 */
export interface LLMServiceOptions {
    /** Configuration des mod√®les support√©s */
    models: LLMModelConfig[];
    
    /** Mod√®le par d√©faut √† utiliser */
    defaultModelId: string;
    
    /** Timeout par d√©faut pour les appels LLM (ms) */
    defaultTimeout: number;
    
    /** Nombre maximum de tentatives en cas d'√©chec */
    maxRetries: number;
    
    /** D√©lai entre les tentatives (ms) */
    retryDelay: number;
    
    /** Si true, active le cache des r√©ponses LLM */
    enableCaching: boolean;
    
    /** R√©pertoire pour persister le cache (optionnel) */
    cacheDirectory?: string;
}

/**
 * Contexte d'un appel LLM avec tous les param√®tres.
 * Utilis√© pour la tra√ßabilit√© et le debug des appels.
 */
export interface LLMCallContext {
    /** Identifiant unique de l'appel */
    callId: string;
    
    /** ID du mod√®le utilis√© */
    modelId: string;
    
    /** Timestamp de d√©but de l'appel */
    startTime: string;
    
    /** Prompt envoy√© au mod√®le */
    prompt: string;
    
    /** Param√®tres sp√©cifiques √† cet appel */
    parameters: Record<string, any>;
    
    /** M√©tadonn√©es additionnelles */
    metadata: Record<string, any>;
}

/**
 * R√©sultat d'un appel LLM avec m√©triques et m√©tadonn√©es.
 * Fournit toutes les informations n√©cessaires pour le suivi et l'optimisation.
 */
export interface LLMCallResult {
    /** Contexte de l'appel original */
    context: LLMCallContext;
    
    /** R√©ponse du mod√®le */
    response: string;
    
    /** Timestamp de fin de l'appel */
    endTime: string;
    
    /** Dur√©e totale de l'appel (ms) */
    duration: number;
    
    /** M√©triques d'utilisation */
    usage: {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
        estimatedCost: number;
    };
    
    /** Si true, la r√©ponse provient du cache */
    fromCache: boolean;
    
    /** Erreur √©ventuelle (null si succ√®s) */
    error?: string;
}

/**
 * Service d'interface avec les mod√®les de langage (LLM).
 * 
 * Ce service centralise tous les appels aux LLM pour le syst√®me de synth√®se.
 * Il fournit une interface unifi√©e, g√®re les erreurs, optimise les performances
 * et offre une tra√ßabilit√© compl√®te des appels.
 * 
 * Fonctionnalit√©s principales :
 * - Interface unifi√©e multi-mod√®les
 * - Gestion robuste des erreurs avec retry
 * - Cache intelligent des r√©ponses
 * - Monitoring et m√©triques d√©taill√©es
 * - Support des appels en lot optimis√©s
 */
export class LLMService {
    private options: LLMServiceOptions;
    
    /**
     * Cache des r√©ponses LLM pour √©viter les re-appels co√ªteux.
     * Cl√©: hash du prompt + param√®tres, Valeur: r√©sultat de l'appel
     */
    private responseCache: Map<string, LLMCallResult> = new Map();
    
    /**
     * Historique des appels pour monitoring et debug.
     * Limit√© en taille pour √©viter une croissance excessive de la m√©moire.
     */
    private callHistory: LLMCallResult[] = [];
    private maxHistorySize = 1000;

    /**
     * Constructeur avec options de configuration.
     *
     * @param options Configuration du service LLM (optionnel, utilise les d√©fauts)
     */
    constructor(options?: Partial<LLMServiceOptions>) {
        this.options = {
            ...DEFAULT_SERVICE_OPTIONS,
            ...options
        };
        this.validateConfiguration();
    }

    // =========================================================================
    // M√âTHODES PRINCIPALES D'APPEL LLM
    // =========================================================================

    /**
     * G√©n√®re une synth√®se de conversation via LLM.
     * 
     * Cette m√©thode est le point d'entr√©e principal pour la g√©n√©ration de synth√®ses.
     * Elle utilise le mod√®le sp√©cifi√© (ou celui par d√©faut) pour analyser le contexte
     * fourni et g√©n√©rer une synth√®se structur√©e.
     * 
     * Phase 1 : M√©thode squelette qui retournera un r√©sultat mock.
     * Phase 3 : Impl√©mentera la logique compl√®te d'appel LLM.
     * 
     * @param context Contexte narratif √† synth√©tiser
     * @param taskId ID de la t√¢che en cours de synth√®se
     * @param modelId Mod√®le √† utiliser (optionnel, utilise le d√©faut sinon)
     * @returns Promise du r√©sultat de l'appel LLM
     */
    async generateSynthesis(
        context: string,
        taskId: string,
        modelId?: string
    ): Promise<LLMCallResult> {
        const activeModelId = modelId || this.options.defaultModelId;
        const modelConfig = this.getModelConfig(activeModelId);
        
        if (!modelConfig) {
            throw new Error(`Mod√®le non configur√©: ${activeModelId}`);
        }

        // Construction du prompt optimis√© pour la synth√®se
        const synthesisPrompt = this.buildSynthesisPrompt(context, taskId, modelConfig.modelName);
        
        try {
            // Appel LLM avec structured output
            const result = await this.callLLM(
                synthesisPrompt,
                activeModelId,
                {
                    response_format: zodResponseFormat(ConversationAnalysisSchema, 'conversation_analysis')
                },
                { taskId, operation: 'synthesis' }
            );

            return result;
        } catch (error) {
            console.error(`Erreur g√©n√©ration synth√®se pour t√¢che ${taskId}:`, error);
            throw new Error(`√âchec g√©n√©ration synth√®se: ${error instanceof Error ? error.message : 'Erreur inconnue'}`);
        }
    }

    /**
     * Condense un ensemble de synth√®ses en un lot unifi√© via LLM.
     * 
     * Cette m√©thode prend plusieurs analyses de conversation et les condense
     * en une synth√®se de haut niveau pour optimiser l'utilisation du contexte.
     * 
     * Phase 1 : M√©thode squelette qui retournera un r√©sultat mock.
     * Phase 3 : Impl√©mentera la logique compl√®te de condensation.
     * 
     * @param analyses Liste des analyses √† condenser
     * @param modelId Mod√®le √† utiliser pour la condensation
     * @returns Promise du r√©sultat de condensation
     */
    async condenseSyntheses(
        analyses: ConversationAnalysis[],
        modelId?: string
    ): Promise<LLMCallResult> {
        const activeModelId = modelId || this.options.defaultModelId;
        const modelConfig = this.getModelConfig(activeModelId);
        
        if (!modelConfig) {
            throw new Error(`Mod√®le non configur√©: ${activeModelId}`);
        }

        if (analyses.length === 0) {
            throw new Error('Aucune analyse √† condenser');
        }

        // Construction du prompt de condensation
        const condensationPrompt = this.buildCondensationPrompt(analyses);
        
        try {
            // Appel LLM avec structured output pour condensation
            const result = await this.callLLM(
                condensationPrompt,
                activeModelId,
                {
                    response_format: zodResponseFormat(CondensedSynthesisBatchSchema, 'condensed_batch')
                },
                {
                    operation: 'condensation',
                    sourceTaskIds: analyses.map(a => a.taskId)
                }
            );

            return result;
        } catch (error) {
            console.error(`Erreur condensation de ${analyses.length} synth√®ses:`, error);
            throw new Error(`√âchec condensation: ${error instanceof Error ? error.message : 'Erreur inconnue'}`);
        }
    }

    /**
     * Effectue un appel LLM g√©n√©rique avec gestion compl√®te des erreurs.
     * 
     * Cette m√©thode est la primitive de base pour tous les appels LLM.
     * Elle g√®re le retry, le cache, les m√©triques et la tra√ßabilit√©.
     * 
     * Phase 3 : Impl√©mentera la logique compl√®te d'appel avec openai-node.
     * 
     * @param prompt Prompt √† envoyer au mod√®le
     * @param modelId Mod√®le √† utiliser
     * @param parameters Param√®tres sp√©cifiques √† cet appel
     * @param metadata M√©tadonn√©es additionnelles
     * @returns Promise du r√©sultat de l'appel
     */
    async callLLM(
        prompt: string,
        modelId?: string,
        parameters?: Record<string, any>,
        metadata?: Record<string, any>
    ): Promise<LLMCallResult> {
        const activeModelId = modelId || this.options.defaultModelId;
        const modelConfig = this.getModelConfig(activeModelId);
        
        if (!modelConfig) {
            throw new Error(`Mod√®le non configur√©: ${activeModelId}`);
        }

        // G√©n√©rer le contexte d'appel
        const callContext: LLMCallContext = {
            callId: this.generateCallId(),
            modelId: activeModelId,
            startTime: new Date().toISOString(),
            prompt,
            parameters: parameters || {},
            metadata: metadata || {}
        };

        // V√©rifier le cache en premier
        const cachedResult = this.getCachedResponse(prompt, parameters || {}, activeModelId);
        if (cachedResult) {
            return cachedResult;
        }

        const startTime = Date.now();

        try {
            // Obtenir le client OpenAI configur√©
            const client = getOpenAIClient();
            
            // Configuration de l'appel
            const openaiParams = {
                model: modelConfig.modelName,
                messages: [{ role: 'user' as const, content: prompt }],
                temperature: 0.1,
                max_tokens: 4000,
                // Ajouter response_format si pr√©sent
                ...(parameters?.response_format ? { response_format: parameters.response_format } : {})
            };

            console.log(`   - process.env.OPENAI_CHAT_MODEL_ID: ${process.env.OPENAI_CHAT_MODEL_ID}`);
            console.log(`   - openaiParams.model: ${openaiParams.model}`);
            console.log(`   - activeModelId: ${activeModelId}`);

            // Ex√©cution de l'appel avec retry automatique d'OpenAI
            const completion = await client.chat.completions.create(openaiParams);
            
            const endTime = Date.now();
            const duration = endTime - startTime;

            // Extraction de la r√©ponse
            const response = completion.choices[0]?.message?.content || '';
            
            // Construction du r√©sultat
            const result: LLMCallResult = {
                context: {
                    ...callContext,
                    // CORRECTION: Utiliser le mod√®le retourn√© par OpenAI (plus fiable)
                    modelId: completion.model || modelConfig.modelName  // Priorit√© √† la r√©ponse OpenAI
                },
                response,
                endTime: new Date().toISOString(),
                duration,
                usage: {
                    promptTokens: completion.usage?.prompt_tokens || 0,
                    completionTokens: completion.usage?.completion_tokens || 0,
                    totalTokens: completion.usage?.total_tokens || 0,
                    estimatedCost: this.calculateCost(
                        completion.usage?.prompt_tokens || 0,
                        completion.usage?.completion_tokens || 0,
                        modelConfig
                    )
                },
                fromCache: false
            };
            
            console.log(`ü§ñ [LLMService] Mod√®le demand√©: ${modelConfig.modelName}, mod√®le utilis√© par OpenAI: ${completion.model}`);

            // Mettre en cache et ajouter √† l'historique
            this.setCachedResponse(prompt, parameters || {}, activeModelId, result);
            this.addToHistory(result);

            return result;

        } catch (error) {
            const endTime = Date.now();
            const duration = endTime - startTime;
            
            // Gestion sp√©cifique des erreurs OpenAI
            let errorMessage = 'Erreur LLM inconnue';
            if (error instanceof OpenAI.APIError) {
                errorMessage = `API OpenAI Error (${error.status}): ${error.message}`;
                console.error(`OpenAI API Error - Status: ${error.status}, Request ID: ${error.headers?.['x-request-id']}`);
            } else if (error instanceof Error) {
                errorMessage = error.message;
            }

            // Construction du r√©sultat d'erreur
            const errorResult: LLMCallResult = {
                context: {
                    ...callContext,
                    // CORRECTION: Utiliser le vrai nom du mod√®le OpenAI, pas l'ID interne
                    modelId: modelConfig.modelName
                },
                response: '',
                endTime: new Date().toISOString(),
                duration,
                usage: {
                    promptTokens: 0,
                    completionTokens: 0,
                    totalTokens: 0,
                    estimatedCost: 0
                },
                fromCache: false,
                error: errorMessage
            };

            this.addToHistory(errorResult);
            throw error;
        }
    }

    // =========================================================================
    // M√âTHODES DE GESTION DES MOD√àLES
    // =========================================================================

    /**
     * Retourne la configuration d'un mod√®le par son ID.
     * 
     * @param modelId ID du mod√®le recherch√©
     * @returns Configuration du mod√®le ou undefined si non trouv√©
     */
    getModelConfig(modelId: string): LLMModelConfig | undefined {
        return this.options.models.find(model => model.modelId === modelId);
    }

    /**
     * Retourne la liste de tous les mod√®les configur√©s.
     * 
     * @returns Tableau des configurations de mod√®les
     */
    getAllModels(): LLMModelConfig[] {
        return [...this.options.models];
    }

    /**
     * V√©rifie si un mod√®le est disponible et configur√©.
     * 
     * @param modelId ID du mod√®le √† v√©rifier
     * @returns boolean true si le mod√®le est disponible
     */
    isModelAvailable(modelId: string): boolean {
        return this.options.models.some(model => model.modelId === modelId);
    }

    // =========================================================================
    // M√âTHODES DE GESTION DU CACHE
    // =========================================================================

    /**
     * V√©rifie si une r√©ponse est disponible dans le cache.
     * 
     * @param prompt Prompt de l'appel
     * @param parameters Param√®tres de l'appel
     * @param modelId Mod√®le utilis√©
     * @returns R√©sultat mis en cache ou null si non trouv√©
     */
    getCachedResponse(
        prompt: string,
        parameters: Record<string, any>,
        modelId: string
    ): LLMCallResult | null {
        if (!this.options.enableCaching) {
            return null;
        }

        const cacheKey = this.generateCacheKey(prompt, parameters, modelId);
        return this.responseCache.get(cacheKey) || null;
    }

    /**
     * Met en cache le r√©sultat d'un appel LLM.
     * 
     * @param prompt Prompt de l'appel
     * @param parameters Param√®tres de l'appel
     * @param modelId Mod√®le utilis√©
     * @param result R√©sultat √† mettre en cache
     */
    setCachedResponse(
        prompt: string,
        parameters: Record<string, any>,
        modelId: string,
        result: LLMCallResult
    ): void {
        if (!this.options.enableCaching) {
            return;
        }

        const cacheKey = this.generateCacheKey(prompt, parameters, modelId);
        this.responseCache.set(cacheKey, result);
    }

    /**
     * Vide le cache des r√©ponses LLM.
     */
    clearCache(): void {
        this.responseCache.clear();
    }

    // =========================================================================
    // M√âTHODES DE MONITORING ET STATISTIQUES
    // =========================================================================

    /**
     * Retourne les statistiques d'utilisation du service.
     * Utile pour le monitoring des performances et co√ªts.
     * 
     * @returns Statistiques d√©taill√©es d'utilisation
     */
    getUsageStats(): {
        totalCalls: number;
        successfulCalls: number;
        failedCalls: number;
        totalCost: number;
        totalTokens: number;
        cacheHitRate: number;
        averageResponseTime: number;
        callsByModel: Record<string, number>;
    } {
        const totalCalls = this.callHistory.length;
        const successfulCalls = this.callHistory.filter(call => !call.error).length;
        const failedCalls = totalCalls - successfulCalls;
        
        const totalCost = this.callHistory.reduce((sum, call) => sum + call.usage.estimatedCost, 0);
        const totalTokens = this.callHistory.reduce((sum, call) => sum + call.usage.totalTokens, 0);
        
        const cachedCalls = this.callHistory.filter(call => call.fromCache).length;
        const cacheHitRate = totalCalls > 0 ? (cachedCalls / totalCalls) * 100 : 0;
        
        const totalDuration = this.callHistory.reduce((sum, call) => sum + call.duration, 0);
        const averageResponseTime = totalCalls > 0 ? totalDuration / totalCalls : 0;
        
        const callsByModel: Record<string, number> = {};
        this.callHistory.forEach(call => {
            callsByModel[call.context.modelId] = (callsByModel[call.context.modelId] || 0) + 1;
        });

        return {
            totalCalls,
            successfulCalls,
            failedCalls,
            totalCost,
            totalTokens,
            cacheHitRate,
            averageResponseTime,
            callsByModel
        };
    }

    /**
     * Retourne l'historique des appels r√©cents.
     * 
     * @param limit Nombre maximum d'appels √† retourner
     * @returns Historique des appels r√©cents
     */
    getCallHistory(limit: number = 100): LLMCallResult[] {
        return this.callHistory.slice(-limit);
    }

    // =========================================================================
    // M√âTHODES PRIV√âES ET UTILITAIRES
    // =========================================================================

    /**
     * Valide la configuration du service au d√©marrage.
     * 
     * @throws Error si la configuration est invalide
     */
    private validateConfiguration(): void {
        if (!this.options.models || this.options.models.length === 0) {
            throw new Error('Au moins un mod√®le doit √™tre configur√©');
        }

        if (!this.options.defaultModelId) {
            throw new Error('Un mod√®le par d√©faut doit √™tre sp√©cifi√©');
        }

        const defaultModel = this.getModelConfig(this.options.defaultModelId);
        if (!defaultModel) {
            throw new Error(`Le mod√®le par d√©faut '${this.options.defaultModelId}' n'est pas configur√©`);
        }

        // Valider chaque mod√®le configur√©
        for (const model of this.options.models) {
            if (!model.modelId || !model.modelName || !model.provider) {
                throw new Error(`Configuration invalide pour le mod√®le: ${JSON.stringify(model)}`);
            }
        }
    }

    /**
     * Construit le prompt optimis√© pour la g√©n√©ration de synth√®se.
     * Int√®gre les instructions contextuelles et le format attendu.
     *
     * @param context Contexte narratif √† synth√©tiser
     * @param taskId ID de la t√¢che en cours
     * @returns Prompt structur√© pour l'analyse LLM
     */
    private buildSynthesisPrompt(context: string, taskId: string, modelName: string): string {
        return `Tu es un expert en analyse de conversations techniques et d√©veloppement logiciel.
Analyse la conversation suivante et g√©n√®re une synth√®se structur√©e compl√®te.

CONTEXTE √Ä ANALYSER :
=====================================
${context}
=====================================

INSTRUCTIONS :
1. Analyse les objectifs de la conversation (ce qui √©tait recherch√©)
2. Identifie la strat√©gie utilis√©e (comment le probl√®me a √©t√© abord√©)
3. √âvalue la qualit√© globale (efficacit√©, clart√©, r√©sultats)
4. Calcule des m√©triques pertinentes (temps, complexit√©, succ√®s)
5. Produis une synth√®se narrative en deux parties :
   - initialContextSummary : r√©sum√© du contexte initial et des pr√©requis
   - finalTaskSummary : synth√®se finale de ce qui a √©t√© accompli

IMPORTANT :
- Sois pr√©cis et factuel
- Base-toi uniquement sur le contenu fourni
- Utilise un langage technique appropri√©
- G√©n√®re du contenu utile pour la documentation et le suivi

TaskId de r√©f√©rence : ${taskId}

IMP√âRATIF : utilise "${modelName}" comme valeur pour le champ llmModelId dans ta r√©ponse JSON`;
    }

    /**
     * Construit le prompt pour condenser plusieurs synth√®ses.
     * Optimis√© pour cr√©er un r√©sum√© de haut niveau coh√©rent.
     *
     * @param analyses Synth√®ses individuelles √† condenser
     * @returns Prompt structur√© pour la condensation
     */
    private buildCondensationPrompt(analyses: ConversationAnalysis[]): string {
        const taskIds = analyses.map(a => a.taskId).join(', ');
        const synthesesSummary = analyses.map((analysis, index) =>
            `SYNTH√àSE ${index + 1} (${analysis.taskId}):\n${analysis.synthesis.finalTaskSummary}`
        ).join('\n\n---\n\n');

        return `Tu es un expert en synth√®se documentaire. Condense les synth√®ses suivantes en un r√©sum√© de haut niveau unifi√©.

SYNTH√àSES √Ä CONDENSER :
${synthesesSummary}

INSTRUCTIONS :
1. Cr√©e un r√©sum√© global coh√©rent qui capture l'essence de toutes les synth√®ses
2. Identifie les patterns et th√®mes communs
3. Conserve les informations cl√©s de chaque synth√®se
4. Produis une synth√®se condens√©e fluide et utile

IDs des t√¢ches source : ${taskIds}
Nombre de synth√®ses √† condenser : ${analyses.length}`;
    }

    /**
     * Calcule le co√ªt estim√© d'un appel LLM bas√© sur les tokens utilis√©s.
     *
     * @param promptTokens Nombre de tokens d'entr√©e
     * @param completionTokens Nombre de tokens de sortie
     * @param modelConfig Configuration du mod√®le avec co√ªts
     * @returns Co√ªt estim√© en dollars US
     */
    private calculateCost(
        promptTokens: number,
        completionTokens: number,
        modelConfig: LLMModelConfig
    ): number {
        const inputCost = promptTokens * modelConfig.costPerInputToken;
        const outputCost = completionTokens * modelConfig.costPerOutputToken;
        return inputCost + outputCost;
    }

    /**
     * G√©n√®re une cl√© de cache pour un appel LLM.
     *
     * @param prompt Prompt de l'appel
     * @param parameters Param√®tres de l'appel
     * @param modelId Mod√®le utilis√©
     * @returns Cl√© de cache unique
     */
    private generateCacheKey(
        prompt: string,
        parameters: Record<string, any>,
        modelId: string
    ): string {
        const data = JSON.stringify({ prompt, parameters, modelId });
        return crypto.createHash('sha256').update(data).digest('hex').substring(0, 32);
    }

    /**
     * G√©n√®re un ID unique pour un appel LLM.
     * 
     * @returns ID unique de l'appel
     */
    private generateCallId(): string {
        return `llm_call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    }

    /**
     * Ajoute un r√©sultat d'appel √† l'historique avec gestion de la taille.
     * 
     * @param result R√©sultat de l'appel √† ajouter
     */
    private addToHistory(result: LLMCallResult): void {
        this.callHistory.push(result);
        
        // Limiter la taille de l'historique pour √©viter une croissance excessive
        if (this.callHistory.length > this.maxHistorySize) {
            this.callHistory = this.callHistory.slice(-this.maxHistorySize);
        }
    }

    // =========================================================================
    // M√âTHODES DE NETTOYAGE
    // =========================================================================

    /**
     * Nettoie les ressources du service.
     * Appel√© lors de l'arr√™t du serveur MCP.
     */
    async cleanup(): Promise<void> {
        // Persister le cache si configur√©
        if (this.options.cacheDirectory && this.options.enableCaching) {
            // TODO Phase 3: Impl√©menter la persistance du cache
        }

        this.clearCache();
        this.callHistory = [];
    }
}